"""
å¢å¼ºç‰ˆå†œè¯æ•°æ®æ¢ç´¢è„šæœ¬
è‡ªåŠ¨æ£€æµ‹å„ç§å¯èƒ½çš„æ–‡ä»¶æ ¼å¼å’Œåç§°
"""

import os
import pandas as pd
from pathlib import Path
import json

def find_data_files():
    """æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„æ•°æ®æ–‡ä»¶"""
    data_dir = Path("data")
    
    if not data_dir.exists():
        print("âŒ data/ ç›®å½•ä¸å­˜åœ¨ï¼")
        print("è¯·ç¡®ä¿æ•°æ®æ–‡ä»¶åœ¨ pesticide-analysis/data/ ç›®å½•ä¸‹")
        return []
    
    # æŸ¥æ‰¾æ‰€æœ‰æ–‡ä»¶
    all_files = list(data_dir.glob("*"))
    print(f"ğŸ“ åœ¨ data/ ç›®å½•ä¸­æ‰¾åˆ° {len(all_files)} ä¸ªæ–‡ä»¶:")
    
    for file in all_files:
        size_kb = file.stat().st_size / 1024
        print(f"  - {file.name} ({size_kb:.1f} KB)")
    
    return all_files

def try_read_file(file_path):
    """å°è¯•ç”¨å¤šç§æ–¹æ³•è¯»å–æ–‡ä»¶"""
    print(f"\nğŸ”„ å°è¯•è¯»å–: {file_path.name}")
    
    # æ–¹æ³•1ï¼šå°è¯•è¯»å–Parquetæ–‡ä»¶
    try:
        df = pd.read_parquet(file_path)
        print(f"  âœ… æˆåŠŸè¯»å–ä¸º Parquet æ–‡ä»¶")
        return df, "parquet"
    except Exception as e1:
        print(f"  âŒ ä¸æ˜¯æ ‡å‡†Parquetæ–‡ä»¶: {e1}")
    
    # æ–¹æ³•2ï¼šå°è¯•è¯»å–CSVæ–‡ä»¶
    try:
        df = pd.read_csv(file_path)
        print(f"  âœ… æˆåŠŸè¯»å–ä¸º CSV æ–‡ä»¶")
        return df, "csv"
    except Exception as e2:
        print(f"  âŒ ä¸æ˜¯CSVæ–‡ä»¶: {e2}")
    
    # æ–¹æ³•3ï¼šå°è¯•è¯»å–Excelæ–‡ä»¶
    try:
        df = pd.read_excel(file_path)
        print(f"  âœ… æˆåŠŸè¯»å–ä¸º Excel æ–‡ä»¶")
        return df, "excel"
    except Exception as e3:
        print(f"  âŒ ä¸æ˜¯Excelæ–‡ä»¶: {e3}")
    
    # æ–¹æ³•4ï¼šå¦‚æœæ˜¯gzipå‹ç¼©çš„Parquetæ–‡ä»¶
    if str(file_path).endswith('.gzip'):
        try:
            df = pd.read_parquet(file_path, engine='pyarrow')
            print(f"  âœ… æˆåŠŸè¯»å–ä¸º gzipå‹ç¼©çš„Parquetæ–‡ä»¶")
            return df, "parquet.gzip"
        except Exception as e4:
            print(f"  âŒ è¯»å–gzipå‹ç¼©æ–‡ä»¶å¤±è´¥: {e4}")
    
    return None, None

def analyze_dataframe(df, file_format):
    """åˆ†ææ•°æ®æ¡†å†…å®¹"""
    print("\n" + "="*60)
    print("ğŸ“Š æ•°æ®è¯¦ç»†åˆ†æ")
    print("="*60)
    
    print(f"æ–‡ä»¶æ ¼å¼: {file_format}")
    print(f"æ•°æ®å½¢çŠ¶: {df.shape[0]} è¡Œ Ã— {df.shape[1]} åˆ—")
    
    print("\nğŸ“‹ æ‰€æœ‰åˆ—å:")
    for i, col in enumerate(df.columns, 1):
        dtype = str(df[col].dtype)
        non_null = df[col].count()
        print(f"  {i:2d}. {col:<30} {dtype:<15} éç©ºå€¼: {non_null}/{df.shape[0]}")
    
    print("\nğŸ‘€ å‰3è¡Œæ•°æ®é¢„è§ˆ:")
    print(df.head(3).to_string())
    
    # æŸ¥æ‰¾æ–‡æœ¬åˆ—
    print("\nğŸ” å¯»æ‰¾æ–‡æœ¬åˆ—:")
    text_columns = []
    for col in df.columns:
        if df[col].dtype == 'object':
            samples = df[col].dropna().head(2)
            if len(samples) > 0:
                text_columns.append(col)
                print(f"\n  ğŸ“„ åˆ— '{col}':")
                for j, sample in enumerate(samples, 1):
                    sample_str = str(sample)
                    preview = sample_str[:100] + "..." if len(sample_str) > 100 else sample_str
                    print(f"     æ ·æœ¬{j}: {preview}")
    
    return text_columns

def save_report(df, text_columns, file_path, file_format):
    """ä¿å­˜åˆ†ææŠ¥å‘Š"""
    report = {
        "file_name": str(file_path.name),
        "file_format": file_format,
        "data_shape": {
            "rows": int(df.shape[0]),
            "columns": int(df.shape[1])
        },
        "columns": list(df.columns),
        "dtypes": {col: str(df[col].dtype) for col in df.columns},
        "text_columns": text_columns,
        "sample_data": {}
    }
    
    # ä¸ºæ¯ä¸ªæ–‡æœ¬åˆ—ä¿å­˜ä¸€äº›æ ·æœ¬
    for col in text_columns[:5]:  # åªå–å‰5ä¸ªæ–‡æœ¬åˆ—
        samples = df[col].dropna().head(3).tolist()
        report["sample_data"][col] = samples
    
    # ä¿å­˜ä¸ºJSON
    with open("data_analysis_report.json", "w", encoding="utf-8") as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    # ä¿å­˜ä¸ºMarkdown
    with open("data_analysis_report.md", "w", encoding="utf-8") as f:
        f.write(f"# å†œè¯æ•°æ®åˆ†ææŠ¥å‘Š\n\n")
        f.write(f"## æ–‡ä»¶ä¿¡æ¯\n")
        f.write(f"- æ–‡ä»¶å: `{file_path.name}`\n")
        f.write(f"- æ ¼å¼: {file_format}\n")
        f.write(f"- æ•°æ®è§„æ¨¡: {df.shape[0]:,} è¡Œ Ã— {df.shape[1]} åˆ—\n\n")
        
        f.write(f"## æ•°æ®åˆ—æ€»è§ˆ\n")
        f.write(f"å…± {df.shape[1]} åˆ—:\n\n")
        for i, col in enumerate(df.columns, 1):
            dtype = str(df[col].dtype)
            non_null = df[col].count()
            f.write(f"{i}. **{col}** - ç±»å‹: `{dtype}`, éç©ºå€¼: {non_null}\n")
        
        f.write(f"\n## æ–‡æœ¬åˆ—è¯¦æƒ…\n")
        f.write(f"æ‰¾åˆ° {len(text_columns)} ä¸ªæ–‡æœ¬åˆ—:\n\n")
        for col in text_columns:
            f.write(f"### {col}\n")
            samples = df[col].dropna().head(2).tolist()
            for j, sample in enumerate(samples, 1):
                f.write(f"æ ·æœ¬{j}: `{str(sample)[:200]}`\n\n")
    
    print(f"\nğŸ“„ æŠ¥å‘Šå·²ä¿å­˜:")
    print(f"  - data_analysis_report.json (æœºå™¨å¯è¯»)")
    print(f"  - data_analysis_report.md (äººå¯è¯»)")

def main():
    print("ğŸ” å¢å¼ºç‰ˆå†œè¯æ•°æ®æ¢ç´¢")
    print("="*60)
    
    # 1. æŸ¥æ‰¾æ‰€æœ‰æ•°æ®æ–‡ä»¶
    files = find_data_files()
    
    if not files:
        print("\nğŸ’¡ å»ºè®®æ“ä½œ:")
        print("1. ç¡®ä¿æ•°æ®æ–‡ä»¶å·²ä¸‹è½½åˆ° Mac")
        print("2. åœ¨ç»ˆç«¯è¿è¡Œ: mkdir -p ~/Desktop/pesticide-analysis/data")
        print("3. å°†æ•°æ®æ–‡ä»¶æ‹–æ”¾åˆ° data/ æ–‡ä»¶å¤¹ä¸­")
        print("4. é‡æ–°è¿è¡Œæœ¬è„šæœ¬")
        return
    
    # 2. å°è¯•è¯»å–æ¯ä¸ªæ–‡ä»¶
    for file_path in files:
        df, file_format = try_read_file(file_path)
        
        if df is not None:
            print(f"\nâœ… æˆåŠŸè¯»å–æ–‡ä»¶: {file_path.name}")
            
            # 3. åˆ†ææ•°æ®
            text_columns = analyze_dataframe(df, file_format)
            
            # 4. ä¿å­˜æŠ¥å‘Š
            save_report(df, text_columns, file_path, file_format)
            
            # 5. ä¿å­˜æ ·æœ¬æ•°æ®ä¸ºCSV
            sample_file = "pesticide_data_sample.csv"
            df.head(100).to_csv(sample_file, index=False, encoding='utf-8')
            print(f"\nğŸ’¾ æ ·æœ¬æ•°æ®å·²ä¿å­˜åˆ°: {sample_file} (å‰100è¡Œ)")
            
            print("\n" + "="*60)
            print("ğŸ¯ ä¸‹ä¸€æ­¥å»ºè®®:")
            print("1. æŸ¥çœ‹ data_analysis_report.md äº†è§£æ•°æ®ç»“æ„")
            print("2. æŸ¥çœ‹ pesticide_data_sample.csv æŸ¥çœ‹å…·ä½“æ•°æ®")
            print("3. æ ¹æ®æ‰¾åˆ°çš„æ–‡æœ¬åˆ—è®¾è®¡DeepSeek APIæé—®")
            break
    else:
        print("\nâŒ æ— æ³•è¯»å–ä»»ä½•æ–‡ä»¶")
        print("\nğŸ’¡ å¯èƒ½åŸå› :")
        print("1. æ–‡ä»¶å·²æŸå - è¯·é‡æ–°ä¸‹è½½")
        print("2. éœ€è¦ç‰¹æ®Šè§£ç  - è¯·è”ç³»æ•™æˆç¡®è®¤æ–‡ä»¶æ ¼å¼")
        print("3. éœ€è¦å®‰è£…é¢å¤–åº“:")
        print("   è¿è¡Œ: pip install pandas pyarrow fastparquet openpyxl")

if __name__ == "__main__":
    main()
    
    
    
    